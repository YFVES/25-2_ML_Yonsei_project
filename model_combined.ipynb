{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7f30b4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# cuBLAS 결정론 모드\n",
    "os.environ[\"CUBLAS_WORKSPACE_CONFIG\"] = \":4096:8\"    # 또는 \":4096:8\"\n",
    "import umap.umap_ as umap\n",
    "\n",
    "import random, numpy as np, torch\n",
    "random.seed(2025); np.random.seed(2025); torch.manual_seed(2025)\n",
    "import scipy.io\n",
    "import numpy as np\n",
    "import json\n",
    "import pandas as pd\n",
    "import pycwt as cwt\n",
    "from scipy.signal import iirnotch, filtfilt\n",
    "from sklearn.linear_model import LinearRegression,Ridge\n",
    "from sklearn.preprocessing import StandardScaler,minmax_scale\n",
    "from sklearn.metrics import r2_score\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cluster import KMeans\n",
    "np.int = int\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "# =============================================================\n",
    "# 하드웨어/재현성 설정\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "\n",
    "\n",
    "# (선택이지만 권장) TF32 비활성화로 추가 일관성\n",
    "torch.backends.cuda.matmul.allow_tf32 = False\n",
    "torch.backends.cudnn.allow_tf32 = False\n",
    "\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "# =============================================================\n",
    "# 경로/기본 파라미터\n",
    "matfilepath=r'D:\\Dropbox\\Shank2_revision\\corrections' #neural/behavioral data of prefrontal cortex in interacting mice\n",
    "positionfilepath=r\"D:\\Dropbox\\Shank2_revision\\PositionData\" #AI-infered position data\n",
    "result=[[] for _ in range(16)]\n",
    "\n",
    "# =============================================================\n",
    "# 공용 유틸\n",
    "def annotation(s1,n1,r1,s2,n2,r2,fs,length):\n",
    "    timestamp=np.arange(0,length,1/fs)\n",
    "    result=np.ones((2,len(timestamp)))\n",
    "\n",
    "    for x in n1:\n",
    "        mask=(x[0]<timestamp)&(timestamp<x[1])\n",
    "        result[0][mask]=1*np.ones(np.sum(mask))\n",
    "    for x in r1:\n",
    "        mask=(x[0]<timestamp)&(timestamp<x[1])\n",
    "        result[0][mask]=1*np.ones(np.sum(mask))    \n",
    "    for x in s1:\n",
    "        mask=(x[0]<timestamp)&(timestamp<x[1])\n",
    "        if np.sum(mask)==0:\n",
    "            result[0][mask]=0*np.ones(np.sum(mask))\n",
    "        result[0][int(x[0]*fs)]=0\n",
    "    for x in n2:\n",
    "        mask=(x[0]<timestamp)&(timestamp<x[1])\n",
    "        result[1][mask]=1*np.ones(np.sum(mask))\n",
    "    for x in r2:\n",
    "        mask=(x[0]<timestamp)&(timestamp<x[1])\n",
    "        result[1][mask]=1*np.ones(np.sum(mask))  \n",
    "    for x in s2:\n",
    "        mask=(x[0]<timestamp)&(timestamp<x[1])\n",
    "        if np.sum(mask)==0:\n",
    "            result[1][mask]=0*np.ones(np.sum(mask))\n",
    "        result[1][int(x[0]*fs)]=0\n",
    "    return result\n",
    "\n",
    "def behavioronehot(behavior,totallength,fps,targetfs):\n",
    "    ts=np.arange(0,totallength,1/fps)\n",
    "    tempresult=np.zeros((len(behavior),len(ts)))\n",
    "    for i in range(len(behavior)):\n",
    "        for j in range(len(behavior[i])):\n",
    "            s=behavior[i][j][0]\n",
    "            e=behavior[i][j][1]\n",
    "            mask=((s<=ts)&(ts<=e))\n",
    "            tempresult[i][mask]=np.ones(np.sum(mask))\n",
    "    dsratio=int(fps/targetfs)\n",
    "    result=np.zeros((len(behavior),int(len(ts)-dsratio)))\n",
    "    for i in range(len(result[0])):\n",
    "        result[:,i]=np.mean(tempresult[:,i:dsratio+i],axis=1)\n",
    "    return result.T\n",
    "def calpcc(lfp1,lfp2):\n",
    "    length=np.min((len(lfp1),len(lfp2)))\n",
    "    lfp1=lfp1[:length]\n",
    "    lfp2=lfp2[:length] \n",
    "    return np.corrcoef(lfp1,lfp2)[0][1]\n",
    "\n",
    "def cate(behaviorarray):\n",
    "    result=[x for x in behaviorarray if len(x)>0]    \n",
    "    result=np.concatenate(result,axis=0)\n",
    "    return result\n",
    "class SimpleRNN(nn.Module):\n",
    "    def __init__(self, input_size=1, hidden_size=16,output_size=21):\n",
    "        super(SimpleRNN, self).__init__()\n",
    "        self.lg=nn.Linear(input_size,output_size)\n",
    "        self.rnn = nn.RNN(input_size, hidden_size, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size,output_size )  # Regression output\n",
    "\n",
    "    def forward(self, x):\n",
    "        out, _ = self.rnn(x)\n",
    "        return self.fc(out[:, -1, :]).unsqueeze(1)  # take last time step\n",
    "      \n",
    "class sequencedataset(TensorDataset):\n",
    "    def __init__(self,x,y):\n",
    "        self.X = torch.tensor(x, dtype=torch.float32)\n",
    "        self.y = torch.tensor(y, dtype=torch.float32).unsqueeze(1)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]     \n",
    "        \n",
    "def RNN_run_validate(Xtr,Ytr,Xva,Yva,lr,batch_size,hidden_size,seq_len,iter,input_length,output_size):\n",
    "    model=SimpleRNN(input_length,hidden_size,output_size).to(device)\n",
    "    criterion=nn.MSELoss()\n",
    "    optimizer=optim.Adam(model.parameters(),lr=lr, weight_decay=1e-4)\n",
    "    traindataset=sequencedataset(Xtr,Ytr)\n",
    "    traindataloader=DataLoader(traindataset,batch_size=batch_size)\n",
    "    valdataset=sequencedataset(Xva,Yva)\n",
    "    valdataloader=DataLoader(valdataset,batch_size=batch_size)\n",
    "    best_loss=np.inf\n",
    "    best_model=None\n",
    "    count=0\n",
    "    tolerance=5\n",
    "    avg_train_loss_array=[]\n",
    "    avg_val_loss_array=[]\n",
    "    avg_best_loss_array=[]\n",
    "    for epoch in range(iter):\n",
    "        model.train()\n",
    "        train_losses=[]\n",
    "        for batch_x,batch_y in traindataloader:\n",
    "            batch_x,batch_y=batch_x.to(device),batch_y.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs=model(batch_x)\n",
    "            loss=criterion(outputs,batch_y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_losses.append(loss.item())\n",
    "        model.eval()\n",
    "        val_losses=[]\n",
    "        with torch.no_grad():\n",
    "            for batch_x,batch_y in valdataloader:\n",
    "                batch_x,batch_y=batch_x.to(device),batch_y.to(device)\n",
    "                val_outputs=model(batch_x)\n",
    "                val_loss=criterion(val_outputs,batch_y)\n",
    "                val_losses.append(val_loss.item())\n",
    "        avg_val_loss=np.mean(val_losses)\n",
    "        avg_train_loss=np.mean(train_losses)\n",
    "        avg_train_loss_array.append(avg_train_loss)\n",
    "        avg_val_loss_array.append(avg_val_loss)\n",
    "        if epoch%100==0:\n",
    "            print(f'train loss:{avg_train_loss}, val loss:{avg_val_loss}')\n",
    "        if avg_val_loss<best_loss:\n",
    "            best_loss=avg_val_loss\n",
    "            best_model=model.state_dict()\n",
    "            avg_best_loss_array.append(best_loss)\n",
    "            count=0\n",
    "        else:\n",
    "            avg_best_loss_array.append(best_loss)\n",
    "            count+=1\n",
    "        if count>tolerance:\n",
    "            break\n",
    "    plt.plot(np.log(avg_train_loss_array))\n",
    "    plt.plot(np.log(avg_val_loss_array))\n",
    "    plt.plot(np.log(avg_best_loss_array))\n",
    "    plt.legend(['train','val','best'])\n",
    "    plt.show()\n",
    "    return best_loss,best_model\n",
    "def RNN_evaluation(model_state,Xte,Yte,hidden_size,input_length,output_size):\n",
    "    model=SimpleRNN(input_length,hidden_size,output_size)\n",
    "    model.load_state_dict(model_state)\n",
    "    model.eval()\n",
    "    R2=[]\n",
    "    with torch.no_grad():\n",
    "        yhat=model(torch.tensor(Xte,dtype=torch.float32)).squeeze(1).detach().cpu().numpy()\n",
    "        print(yhat.shape)\n",
    "        print(Yte.shape)\n",
    "        for i in range(len(yhat[0])):\n",
    "            if np.var(yhat[:,i])*np.var(Yte[:,i]):\n",
    "                R2.append(r2_score(yhat[:,i],Yte[:,i]))\n",
    "            else:\n",
    "                R2.append(0)\n",
    "    return R2\n",
    "f0 = 60.0\n",
    "Q = 30.0 \n",
    "def notch(x,b,a):\n",
    "    x=x.T\n",
    "    for i in range(len(x)):\n",
    "        x[i]=filtfilt(b,a,x[i])\n",
    "    return np.nanmean(x,axis=0)\n",
    "def devide_k(n,k):\n",
    "    length=n//k\n",
    "    leftover=n%k\n",
    "    result=[]\n",
    "    for i in range(k):\n",
    "        result.append(np.arange(i*length,(i+1)*length+(1 if i<leftover else 0),1))\n",
    "    return result\n",
    "def create_seq(index_list,seq_len):\n",
    "    result=[]\n",
    "    for index in index_list:\n",
    "        edited_index=index[int(seq_len/2):-int(seq_len/2)]\n",
    "        for i in edited_index:\n",
    "            result.append(np.arange(i-int(seq_len/2),i+int(seq_len/2)))\n",
    "    return result\n",
    "from sklearn.metrics import accuracy_score, r2_score\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import torch, torch.nn as nn\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from sklearn.metrics import silhouette_score\n",
    "from matplotlib.collections import LineCollection\n",
    "from matplotlib.colors import Normalize\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.svm import SVC\n",
    "def nested_cv_rnn_raw(X,Y,*,outer_k=5,lr=0.0001,batch_size=128,hidden_size=64,seq_len=50,iter=100):\n",
    "    X=np.array(X)\n",
    "    Y=np.array(Y)\n",
    "    n_samples=len(X)\n",
    "    result=[]\n",
    "    input_length=X.shape[-1]\n",
    "    output_size=Y.shape[-1]\n",
    "    indexlist=devide_k(n_samples,outer_k)\n",
    "    inner_k=outer_k-1\n",
    "    for iii in range(outer_k):\n",
    "        testindex=[indexlist[iii]]\n",
    "        innerindex=[]\n",
    "        for i in range(outer_k):\n",
    "            if i!=iii:\n",
    "                innerindex.append(indexlist[i])\n",
    "        total_best_loss=np.inf\n",
    "        total_best_model=None\n",
    "        for ii in range(inner_k):\n",
    "            valindex=[innerindex[ii]]\n",
    "            trainindex=[]\n",
    "            for i in range(inner_k):\n",
    "                if i!=ii:\n",
    "                    trainindex.append(innerindex[i])\n",
    "            trainseq=create_seq(trainindex,seq_len)\n",
    "            valseq=create_seq(valindex,seq_len)\n",
    "            X_tr=np.array([X[seq] for seq in trainseq])\n",
    "            Y_tr=np.array([(Y[seq])[-1] for seq in trainseq])\n",
    "            X_va=np.array([X[seq] for seq in valseq])\n",
    "            Y_va=np.array([(Y[seq])[-1] for seq in valseq])\n",
    "            scX=StandardScaler().fit(X_tr.reshape(-1,(X_tr).shape[-1]))\n",
    "            scY=StandardScaler().fit(Y_tr.reshape(-1,(Y_tr).shape[-1]))\n",
    "            Xtr=scX.transform(X_tr.reshape(-1,X_tr.shape[-1])).reshape(X_tr.shape)\n",
    "            Ytr=scY.transform(Y_tr.reshape(-1,Y_tr.shape[-1])).reshape(Y_tr.shape)\n",
    "            Xva=scX.transform(X_va.reshape(-1,X_va.shape[-1])).reshape(X_va.shape)\n",
    "            Yva=scY.transform(Y_va.reshape(-1,Y_va.shape[-1])).reshape(Y_va.shape)\n",
    "            best_loss,best_model=RNN_run_validate(Xtr,Ytr,Xva,Yva,lr=lr,batch_size=batch_size,hidden_size=hidden_size,seq_len=seq_len,iter=iter,input_length=input_length,output_size=output_size)\n",
    "            if best_loss<total_best_loss:\n",
    "                total_best_loss=best_loss\n",
    "                total_best_model=best_model\n",
    "        trainvalseq=create_seq(innerindex,seq_len)\n",
    "        testseq=create_seq(testindex,seq_len)\n",
    "        X_tv=np.array([X[seq] for seq in trainvalseq])\n",
    "        Y_tv=np.array([(Y[seq])[-1] for seq in trainvalseq])\n",
    "        X_te=np.array([X[seq] for seq in testseq])\n",
    "        Y_te=np.array([(Y[seq])[-1] for seq in testseq])    \n",
    "        scX=StandardScaler().fit(X_tv.reshape(-1,(X_tv).shape[-1]))\n",
    "        scY=StandardScaler().fit(Y_tv.reshape(-1,(Y_tv).shape[-1]))\n",
    "        Xtv=scX.transform(X_tv.reshape(-1,X_tv.shape[-1])).reshape(X_tv.shape)\n",
    "        Ytv=scY.transform(Y_tv.reshape(-1,Y_tv.shape[-1])).reshape(Y_tv.shape)\n",
    "        Xte=scX.transform(X_te.reshape(-1,X_te.shape[-1])).reshape(X_te.shape)\n",
    "        Yte=scY.transform(Y_te.reshape(-1,Y_te.shape[-1])).reshape(Y_te.shape)\n",
    "        result.append(RNN_evaluation(total_best_model,Xte,Yte,hidden_size,input_length,output_size))\n",
    "    return np.mean(result)\n",
    "def slidingwindowfft(wave,dt,finalfreq):\n",
    "    ratio=int(1/(dt*finalfreq))\n",
    "    freq=np.fft.fftfreq(ratio,dt)\n",
    "    length=int(len(wave)/ratio)\n",
    "    lfp=np.zeros((len(freq),length),dtype=np.complex128)\n",
    "    for i in range(length):\n",
    "        lfp[:,i]=np.fft.fft(wave[i*ratio:(i+1)*ratio])    \n",
    "    return lfp,freq\n",
    "finalfreq=1\n",
    "\n",
    "result=[[],[],[],[]]\n",
    "for file in os.listdir(matfilepath):\n",
    "    if (not file.endswith('LFP.mat')) & (file.endswith('.mat')):\n",
    "#loading LFP(Local Field Potential) data, 2000Hz, 10min\n",
    "        Kfilepath=os.path.join(matfilepath,file)\n",
    "        LFPfilepath=Kfilepath[:-4]+'_LFP.mat'\n",
    "        if not os.path.exists(LFPfilepath):\n",
    "            continue\n",
    "        else:\n",
    "            LFP=scipy.io.loadmat(LFPfilepath)\n",
    "        fs=LFP['SamplingFreq'][0][0]\n",
    "        lfpraw1=LFP['raw_signal_final'][0][0]\n",
    "\n",
    "        lfpraw2=LFP['raw_signal_final'][0][1]\n",
    "        \n",
    "\n",
    "#loading genotype data: WT/KO(autism model mouse)\n",
    "        K=scipy.io.loadmat(Kfilepath)['K']\n",
    "        ID1=file[:4]+K['ID'][0][0][0][0][0]\n",
    "        ID2=file[:4]+K['ID'][0][0][0][1][0]\n",
    "        if K['ID'][0][0][0][0][0].endswith('WT'):\n",
    "            type1=0\n",
    "        else:\n",
    "            type1=1\n",
    "        if K['ID'][0][0][0][1][0].endswith('WT'):\n",
    "            type2=0\n",
    "        else:\n",
    "            type2=1\n",
    "        if K['ID'][0][0][0][0][0].endswith('WT'):\n",
    "            if type1+type2==1:\n",
    "                m1=1\n",
    "            else:\n",
    "                m1=0\n",
    "        else:\n",
    "            if type1+type2==1:\n",
    "                m1=2\n",
    "            else:\n",
    "                m1=3\n",
    "        if K['ID'][0][0][0][1][0].endswith('WT'):\n",
    "            if type1+type2==1:\n",
    "                m2=1\n",
    "            else:\n",
    "                m2=0\n",
    "        else:\n",
    "            if type1+type2==1:\n",
    "                m2=2\n",
    "            else:\n",
    "                m2=3\n",
    "        \n",
    "#loading position data: inferred using DEEPLABCUT, 30Hz, for 2 mice\n",
    "#normalizing, differentiating, and simple mathmatical calculation to achieve kinematic features\n",
    "        csvfilepath=os.path.join(positionfilepath,file[:-4]+'.csv')\n",
    "        if os.path.exists(csvfilepath):\n",
    "            positiondata1=pd.read_csv(csvfilepath)[['Center_1_x','Center_1_y']].to_numpy()\n",
    "            positiondata2=pd.read_csv(csvfilepath)[['Center_2_x','Center_2_y']].to_numpy()\n",
    "            xx1=minmax_scale(positiondata1[:,0])\n",
    "            yy1=minmax_scale(positiondata1[:,1])\n",
    "            xx2=minmax_scale(positiondata2[:,0])\n",
    "            yy2=minmax_scale(positiondata2[:,1])\n",
    "            positiondata1[:,0]=xx1\n",
    "            positiondata1[:,1]=yy1\n",
    "            positiondata2[:,0]=xx2\n",
    "            positiondata2[:,1]=yy2\n",
    "            distance=np.sqrt(np.sum((positiondata1-positiondata2)**2,axis=1))\n",
    "            vvdis=distance[1:]-distance[:-1]\n",
    "            vvx1=xx1[1:]-xx1[:-1]\n",
    "            vvx2=xx2[1:]-xx2[:-1]\n",
    "            vvy1=yy1[1:]-yy1[:-1]\n",
    "            vvy2=yy2[1:]-yy2[:-1]\n",
    "            displacement1=positiondata1[1:,:]-positiondata1[:-1,:]\n",
    "            velocity1=np.sqrt(displacement1[:,0]**2+displacement1[:,1]**2)\n",
    "            displacement2=positiondata2[1:,:]-positiondata2[:-1,:]\n",
    "            velocity2=np.sqrt(displacement2[:,0]**2+displacement2[:,1]**2)\n",
    "            dsrate=int(30/finalfreq)\n",
    "            x1=np.zeros(int((len(velocity1)-dsrate)))\n",
    "            x2=np.zeros(int((len(velocity1)-dsrate)))\n",
    "            y1=np.zeros(int((len(velocity1)-dsrate)))\n",
    "            y2=np.zeros(int((len(velocity1)-dsrate)))\n",
    "            v1=np.zeros(int((len(velocity1)-dsrate)))\n",
    "            vx1=np.zeros(int((len(velocity1)-dsrate)))\n",
    "            vx2=np.zeros(int((len(velocity1)-dsrate)))\n",
    "            vy1=np.zeros(int((len(velocity1)-dsrate)))\n",
    "            vy2=np.zeros(int((len(velocity1)-dsrate)))\n",
    "            vdis=np.zeros(int((len(velocity1)-dsrate)))\n",
    "            v2=np.zeros(int((len(velocity1)-dsrate)))\n",
    "            dis=np.zeros(int((len(velocity1)-dsrate)))\n",
    "            \n",
    "            for i in range(len(v1)):\n",
    "                x1[i]=np.mean(xx1[i:i+dsrate])\n",
    "                y1[i]=np.mean(yy1[i:i+dsrate])\n",
    "                x2[i]=np.mean(xx2[i:i+dsrate])\n",
    "                y2[i]=np.mean(yy2[i:i+dsrate])\n",
    "                v1[i]=np.mean(velocity1[i:i+dsrate])\n",
    "                v2[i]=np.mean(velocity2[i:i+dsrate])\n",
    "                dis[i]=np.mean(distance[i:i+dsrate])\n",
    "                vdis[i]=np.mean(vvdis[i:i+dsrate])\n",
    "                vx1[i]=np.mean(vvx1[i:i+dsrate])\n",
    "                vy1[i]=np.mean(vvy1[i:i+dsrate])\n",
    "                vx2[i]=np.mean(vvx2[i:i+dsrate])\n",
    "                vy2[i]=np.mean(vvy2[i:i+dsrate])\n",
    "        else:\n",
    "            continue\n",
    "        \n",
    "        cx1=x1-0.5\n",
    "        cx2=x2-0.5\n",
    "        cy1=y1-0.5\n",
    "        cy2=y2-0.5\n",
    "        c1=np.sqrt(cx1**2+cy1**2)\n",
    "        c2=np.sqrt(cx2**2+cy2**2)\n",
    "#loading manualing annotated data: 30Hz, 10min\n",
    "#grouped as social/nonsocial/rest behavior\n",
    "        s1behavior=[np.array(K['SocialBehavior'][0][0][0][1][0][0]),np.array(K['SocialBehavior'][0][0][0][1][0][1]),np.array(K['SocialBehavior'][0][0][0][1][0][2]),np.array(K['SocialBehavior'][0][0][0][1][0][3])]\n",
    "        s1behavior=[x for x in s1behavior if len(x)]\n",
    "        selfsocialbehavior=np.concatenate(s1behavior,axis=0)\n",
    "        ns1behavior=[np.array(K['NonSocialBehavior'][0][0][0][1][0][0]),np.array(K['NonSocialBehavior'][0][0][0][1][0][1]),np.array(K['NonSocialBehavior'][0][0][0][1][0][2]),np.array(K['Exploring'][0][0][0][1][0][0]),np.array(K['Exploring'][0][0][0][1][0][0]),np.array(K['Exploring'][0][0][0][1][0][1]),np.array(K['Exploring'][0][0][0][1][0][2])]\n",
    "        ns1behavior=[x for x in ns1behavior if len(x)>0]\n",
    "        selfnonsocialbehavior=np.concatenate(ns1behavior,axis=0)\n",
    "        s2behavior=[np.array(K['SocialBehavior'][0][0][0][2][0][0]),np.array(K['SocialBehavior'][0][0][0][2][0][1]),np.array(K['SocialBehavior'][0][0][0][2][0][2]),np.array(K['SocialBehavior'][0][0][0][2][0][3])]\n",
    "        s2behavior=[x for x in s2behavior if len(x)>0]\n",
    "        othersocialbehavior=np.concatenate(s2behavior,axis=0)\n",
    "        ns2behavior=[np.array(K['NonSocialBehavior'][0][0][0][2][0][0]),np.array(K['NonSocialBehavior'][0][0][0][2][0][1]),np.array(K['NonSocialBehavior'][0][0][0][2][0][2]),np.array(K['Exploring'][0][0][0][2][0][0]),np.array(K['Exploring'][0][0][0][2][0][0]),np.array(K['Exploring'][0][0][0][2][0][1]),np.array(K['Exploring'][0][0][0][2][0][2])]\n",
    "        ns2behavior=[x for x in ns2behavior if len(x)>0]\n",
    "        othernonsocialbehavior=np.concatenate(ns2behavior,axis=0)\n",
    "        otherrestbehavior=np.array(K['Immobile'][0][0][0][2][0][0]) if len(K['Immobile'][0][0][0][2][0][0])>0 else np.zeros((1,3))\n",
    "        selfrestbehavior=np.array(K['Immobile'][0][0][0][1][0][0]) if len(K['Immobile'][0][0][0][1][0][0])>0 else np.zeros((1,3))   \n",
    "#validating that the recording and analysis has been done for 10mins\n",
    "        if len(K['Event'][0][0][0][3])>1:\n",
    "            starttime=K['Event'][0][0][0][3][0]\n",
    "            endtime=K['Event'][0][0][0][3][1]\n",
    "        else:\n",
    "            starttime=K['Event'][0][0][0][3][0][0]\n",
    "            endtime=K['Event'][0][0][0][3][0][1]   \n",
    "        totaltime=float((endtime-starttime)/(1e6))\n",
    "        totaltime=np.min([totaltime,600])\n",
    "\n",
    "#loading spike(neuron activity timestamps data)->converting into firing rate(how many activities per time bin) data   \n",
    "        SPK=K['SPK'][0][0]\n",
    "        num=0\n",
    "        s1=[]\n",
    "        s2=[]\n",
    "        for iii in range(len(SPK)):\n",
    "            spike=SPK[iii][0]\n",
    "            if len(spike)>1:\n",
    "                spike=spike-starttime/1e6\n",
    "                ts=np.arange(0,totaltime,1/30)\n",
    "                if np.sum((spike>0)&(spike<totaltime))/totaltime >0.5:\n",
    "                    temp=[]\n",
    "                    for i in range(len(ts)-1):\n",
    "                        temp.append(np.sum((ts[i]<=spike)&(spike<ts[i]+1/finalfreq))*finalfreq)\n",
    "                    s1.append(temp/np.mean(temp))\n",
    "            spike=SPK[iii][1]\n",
    "            if len(spike)>1:\n",
    "                spike=spike-starttime/1e6\n",
    "                ts=np.arange(0,totaltime,1/30)\n",
    "                if np.sum((spike>0)&(spike<totaltime))/totaltime >0.5:\n",
    "                    temp=[]\n",
    "                    for i in range(len(ts)-1):\n",
    "                        temp.append(np.sum((ts[i]<=spike)&(spike<ts[i]+1/finalfreq))*finalfreq)\n",
    "                    s2.append(temp/np.mean(temp))\n",
    "#checkpoint: the number of neuron recorded varies session by session, so insures the presence of neurons in both mice        \n",
    "        if (len(s1)>1)*(len(s2)>1)==0:\n",
    "            continue\n",
    "\n",
    "        s1=np.array(s1).T\n",
    "        s2=np.array(s2).T\n",
    "#convert LFP raw trace->LFP wavelet power(square of instaneous amplitude) trace for different bands\n",
    "        dt = 1/fs  # 샘플링 간격 (s)\n",
    "        t = np.arange(0, totaltime, dt)  # 10초\n",
    "\n",
    "        # 2. Morlet wavelet transform\n",
    "        mother = cwt.wavelet.Morlet(6)  # Morlet wavelet, w0=6\n",
    "        s0 = 2 * dt                    # 최소 scale\n",
    "        dj = 1/12                       # scale step\n",
    "        J = 7 / dj                      # scale 개수\n",
    "\n",
    "        wave1, scales, freqs, coi, fft, fftfreqs = cwt.cwt(lfpraw1, dt, dj, s0, J, mother)   \n",
    "        wave2, scales, freqs, coi, fft, fftfreqs = cwt.cwt(lfpraw2, dt, dj, s0, J, mother)      \n",
    "        # Wavelet → 밴드 파워 → 10Hz 다운샘플 → 표준화\n",
    "        # wave1,wfreqs=slidingwindowfft(lfpraw1,1/fs,finalfreq)\n",
    "        # wave2,wfreqs=slidingwindowfft(lfpraw2,1/fs,finalfreq)   \n",
    "        # # wave1=artifactcorrection(wave1)\n",
    "        # # wave2=artifactcorrection(wave2)\n",
    "        band=[[4,12],[13,30],[31,80],[4,100]]\n",
    "        lfp1=[[],[],[],[]]\n",
    "        lfp2=[[],[],[],[]]\n",
    "\n",
    "        for i in range(4):\n",
    "            freqmask=(band[i][0]<=freqs)&(band[i][1]>=freqs)\n",
    "            lfp1[i]=(np.sum(np.abs(wave1[freqmask])**2,axis=0)).flatten()\n",
    "            lfp2[i]=(np.sum(np.abs(wave2[freqmask])**2,axis=0)).flatten()\n",
    "\n",
    "        lfp1=np.array(lfp1)/np.max(lfp1)\n",
    "        lfp2=np.array(lfp2)/np.max(lfp2)\n",
    "        lfp1=lfp1.T\n",
    "        lfp2=lfp2.T\n",
    "        lfp1ds=np.zeros((int((len(lfp1)-int(fs/finalfreq))),4))\n",
    "        lfp2ds=np.zeros((int((len(lfp2)-int(fs/finalfreq))),4))\n",
    "        for i in range(len(lfp1ds)):\n",
    "            lfp1ds[i,:]=np.mean(lfp1[i:i+int(fs/finalfreq),:],axis=0)\n",
    "            lfp2ds[i,:]=np.mean(lfp2[i:i+int(fs/finalfreq),:],axis=0)\n",
    "#onehotencoding of mannually annotated behavior data\n",
    "        b1=behavioronehot([selfsocialbehavior,selfnonsocialbehavior,selfrestbehavior],totaltime,30,finalfreq)\n",
    "        b2=behavioronehot([othersocialbehavior,othernonsocialbehavior,otherrestbehavior],totaltime,30,finalfreq)\n",
    "\n",
    "\n",
    "        minlen=np.min([len(v1),len(v2),len(b1),len(b2),len(dis),len(x1),len(x2),len(y1),len(y2),len(s1),len(s2),len(lfp1ds),len(lfp2ds)])\n",
    "#final scaling and fragmenting time-serious position data\n",
    "        x1=(x1[:minlen].reshape(-1,1))\n",
    "        x2=(x2[:minlen].reshape(-1,1))\n",
    "        y1=(y1[:minlen].reshape(-1,1))\n",
    "        y2=(y2[:minlen].reshape(-1,1))\n",
    "        v1=(85*(v1[:minlen].reshape(-1,1)))\n",
    "        v2=(85*(v2[:minlen].reshape(-1,1)))\n",
    "        dis=(dis[:minlen].reshape(-1,1))\n",
    "        time=np.linspace(0,1,minlen).reshape(-1,1)\n",
    "        b1=b1[:minlen]\n",
    "        b2=b2[:minlen]\n",
    "        s1=s1[:minlen]\n",
    "        s2=s2[:minlen]\n",
    "        lfp1ds=minmax_scale(lfp1ds[:minlen])\n",
    "        lfp2ds=minmax_scale(lfp2ds[:minlen])\n",
    "        vx1=85*vx1[:minlen].reshape(-1,1)\n",
    "        vx2=85*vx2[:minlen].reshape(-1,1)\n",
    "        vy1=85*vy1[:minlen].reshape(-1,1)\n",
    "        vy2=85*vy2[:minlen].reshape(-1,1)\n",
    "        vdis=85*vdis[:minlen].reshape(-1,1)\n",
    "        c1=3.1*(c1[:minlen].reshape(-1,1))\n",
    "        c2=3.1*(c2[:minlen].reshape(-1,1))\n",
    "        seq_len = 10\n",
    "        outer_k = 5\n",
    "\n",
    "#concatenating all data into single data matrix, excuting training/validation/test\n",
    "        Y_raw=np.concatenate([x1,x2,y1,y2,v1,v2,dis,vx1,vx2,vy1,vy2,vdis,c1,c2,lfp1ds,lfp2ds],axis=1)\n",
    "        name_dic=np.array(['x1','x2','y1','y2','v1','v2','dis','vx1','vx2','vy1','vy2','vdis','c1','c2','1theta','1beta','1gamma','1wide','2theta','2beta','2gamma','2wide'])\n",
    "        try:\n",
    "            X_raw=np.log(s1)\n",
    "            mean_r2=nested_cv_rnn_raw(X_raw,Y_raw,outer_k=outer_k,lr=1e-4,batch_size=32,hidden_size=32,seq_len=2,iter=1000)\n",
    "            mean_r2=np.array(mean_r2)\n",
    "            print(mean_r2)\n",
    "            print(name_dic[mean_r2>0])\n",
    "            print(mean_r2[mean_r2>0])\n",
    "\n",
    "            X_raw=np.log(s2)\n",
    "            mean_r2=nested_cv_rnn_raw(X_raw,Y_raw,outer_k=outer_k,lr=1e-4,batch_size=32,hidden_size=32,seq_len=2,iter=1000)\n",
    "            mean_r2=np.array(mean_r2)\n",
    "            print(mean_r2)\n",
    "            print(name_dic[mean_r2>0])\n",
    "            print(mean_r2[mean_r2>0])\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            continue\n",
    "pd.DataFrame(result).to_csv(\"R2_RNN_LFP_nestedCV.csv\")        \n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LFPmodel",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
